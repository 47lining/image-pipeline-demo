# As of end of sprint "ResourceShare 0.7", follow these steps to get the image-pipeline-demo to work
1 - Set up account and cage using normal nucleator commands
2 - Open the nat's security group. Add an inbound rule to allow traffic on port 5439 from anywhere("0.0.0.0/0")
3 - Add an outbound rule to allow traffic on port 5439 to anywhere("0.0.0.0/0")
Note - Steps 2 and 3 are documented in backlog task "RS-629". Once this task is completed these steps will not be necessary
4 - The 47Lining test6(587174233256) account holds the two data source buckets. Copy the data in these buckets into two buckets in the account that will be running the pipeline demo
4 - Run the choreographer.sh shell script located in the bin folder of the image-pipeline-demo repo
6 - Enter various arguments along the way that the shell script asks for
7 - At the end, the sqs message will be sent to the distributor beanstalk to load in the first dataset. This will take about 20 minutes and once done, the two looker urls below should show the first piece of the story
8 - During the live demo, you can run the sendqueuemessage.sh script located in the distributor folder of the image-pipeline-demo repo. Once the arguments are entered, this will send the message to kick off the second data copy. This will take about 9 minutes to run and once complete the two looker urls below should be updated to see the new piece of the story

Looker URL's
- https://47lining.looker.com/embed/public/looks/v9r4jVtcwhRWp5H7qmZ3JQPNQ8mvV5C3
- https://47lining.looker.com/embed/public/looks/gXQ64YfCQVPyZQmTQ6sqmwdZxZZXbWDH

## eb-py-flask-signup-worker
This Python sample application illustrates the worker role functionality of AWS Elastic Beanstalk. It is designed to process messages generated by the orchestrator. Messages contain a date, and the worker app reads a specification and (potentially) generates an image.

### Message Format
To test the worker app without the frontend, you can manually enqueue messages of the following format:

{
    "key" : "long_random_string",
    "date" : "2015-04-15 00:00:00"
}

{
    "redshift_initial_copy" : "Running_intial_data_copy_command"
}

{
    "redshift_second_copy" : "Running_second_data_copy_command"
}
